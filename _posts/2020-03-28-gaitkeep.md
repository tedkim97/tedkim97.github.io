---
layout: post
title: GaitKeep&#58; Multi-Modal Approach to Gait Authentication
subtitle: A talk-through of an attempted authentication system
published: false
enable_latex: false
permalink: /gaitkeep
frontpage: true
technical: true
funstuff: false
---

Background
Engineering & Design 
Classification
Conclusion

# Introduction to GaitKeep
This post is about a group project that our team completed for a mobile computing class. GaitKeep was a multi-modal approach to a biometric authentication that verified users through a combination of accelerometers (from a mobile device) and video-camera to increase security and consistency if gait authentication. GaitKeep was only 75-85% finished, but I thought it's still important to document our work & decisions we made.  

**1) <a href="#background">Background</a>**  
**2) <a href="#engineering--design">Engineering & Design</a>**   
**3) <a href="#classification">Classification System</a>**  
**4) <a href="#conclusion--results">Conclusion & Results</a>**  

# Background

### What is Gait Authentication?
Gait authentication is the process of verifying user's identity through their walking patterns with sensors like pressure-sensing plates, accelerometers, gyroscopes, or a camera. This could be framed as gait identification (classifying an individual based on their walk) - where the goal is to maximize accuracy. However, authentication wants to confirm that the proposed identity, Alice is actually Alice by analyzing her gait. On the other hand, identification is involves identifying a given gait pattern and labeling it as Alice. 

##### Why Gait Authentication?
Your gait is a "[biometric](https://en.wikipedia.org/wiki/Biometrics#Likelihood_of_full_governmental_disclosure)" measure - meaning that it is a characteristic that we can observe or compute from your patterns. Biometrics are ideal for authentication because instead of relying on a password (something you remember) or an ID card (something you have), it relies on your **identity**. By making authentication about your identity, rather than something you need to memorize or own, you can prevent attackers who have obtained your passwords or ID cards. 

Biometrics are becoming increasingly popular because they offer improvements where traditional authentication (passwords, pins, etc) fails. In addition, there is a usability aspect to biometrics as well - where tapping your fingerprint or scanning your face is more "seamless" than trying to remember a password or trying to scan a card. 

##### Common Exploits for Common Authentication
The reasons why we care about Biometric authentication is that it **can** be more secure than traditional authentication systems (I say "can" because *bad* biometric authentication can exist). Aside from not being careful with your username and password, there are plenty of well known attacks for breaking into your accounts, whether it involves social engineering or smart tricks. 

For example, say that account information was leaked because of a data breach at [dropbox](https://www.cpomagazine.com/cyber-security/the-data-dump-of-2-2-billion-breached-accounts-what-you-need-to-know/). If you use the same password and email for your dropbox account for your social media, banking, and online shopping you might be in trouble! People who have a copy of the leaked files will just try all the possible username and password on other services! As a result, even though you changed your dropbox password - all your other user accounts have been compromised (and you need to change all the password to all your other accounts). This is why it's advisable to have multiple passwords (but who really wants to memorize all those passwords)? 

*You can check services like ["have I been pwned?"](https://haveibeenpwned.com) that will see if your email address has been leaked in any of these ways*

This is just one of the easy ways that people can exploit username/password authentication systems, but there are more and more ways. [People can attack your accounts through your SIM](https://www.vice.com/en_us/article/vbqax3/hackers-sim-swapping-steal-phone-numbers-instagram-bitcoin), [keyloggers](https://en.wikipedia.org/wiki/Keystroke_logging), and clever password cracking. Sometimes the companies holding your information might make the mistake. [Twitter was compromised through a social engineering attack](https://www.nytimes.com/2020/07/31/technology/twitter-hack-arrest.html) and [CapitalOne was breached by a misconfigured firewall](https://krebsonsecurity.com/tag/capital-one-breach/) (there are just a few and recent examples, there are a lot more if you look). 

##### Why Multi-modal Authentication (why use more than one sensor)?
One of the influences on our project design was the success of multiple sensors in RAPID ([Rapid: A Multimodal and Device-free Approach Using Noise Estimation for Robust Person Identification](https://dl.acm.org/doi/10.1145/3130906)). RAPID was motivated by the success of device-free sensing that focus on using WiFi to identify individuals (Some examples are “[Gait recognition using wifi signals](https://dl.acm.org/doi/10.1145/2971648.2971670)”, “[WiWho](https://ieeexplore.ieee.org/document/7460727)”, “[WiFi-ID](https://ieeexplore.ieee.org/document/7536315)”). However, these systems were incredibly sensitive to noise, and the authors of the paper believed that they could improve identification performance by using an additional acoustic sensor (microphone). The acoustic sensor estimated system and environment noise (a “noise estimation” process) to see the noises influence on the wifi-signals. Afterwards, they processed the signals and performed the traditional gait analysis used in other models. As a result our group thought that there was potential to augment computer-vision based gait classification with accelerometer data to create more accurate classifications. 

Besides accuracy, we thought that using a multi-modal approach to authentication would provide higher security through more stringent requirements. Despite its strengths, biometric security can still have weaknesses and potential attacks. For example, the [fingerprint sensor on the galaxy S10 was tricked by a 3D printed finger](https://www.theverge.com/2019/4/7/18299366/samsung-galaxy-s10-fingerprint-sensor-fooled-3d-printed-fingerprint). You could argue that this attack is very unrealistic or that important security facilities would have a more sophisticated sensor (and I agree about these), but these exists viable attacks. By requiring additional biometric data to be collected, we can **attempt** to increase the security of our system. 

### Biometric Authentication - a quick review 
Biometric authentication systems verifies its users through a unique physiological or behavioral characteristics (some popular examples include fingerprint login, face-id, etc). Naturally, you might notice that there are some important factors that determine the viability of a biometric signature.

Factors determining the usefulness of a biometric system include: 
- **Universality** - does everyone have this trait?
- **Uniqueness** - is this trait unique to every individual?
- **Permanence** - does this trait stay the same, or does it change over time?
- **Measurability/Collectability** - is this trait feasible to measure or compute?
- **Performance** - how fast and accurate are systems using this trait?
- **Acceptability** - will people accept this biometric trait being used?
- **Circumvention** - how easy is it to imitate this trait?

For example, **fingerprints** are Universal
- Universal (almost everyone has one)
- Unique (everyone's fingerprint is unique)
- Permanent (you fingerprint shouldn't change over time)
- Measurable (we can accurately track and measure a fingerprint with a special sensor)
- Fast (seems to work fine - not an expert on the engineering)
- Acceptability (most people are okay with having their fingerprint taken)
- Hard to imitate (No matter what spy movie or show tells you, it is hard to duplicate a fingerprint)

If we think about gait patterns, they also fulfill the requirements! Although, the speed and acceptability of gait authentication depends on the implementation.

# Engineering & Design

### The Goal
Our goal was to improve gait authentication (i.e increase accuracy and security) by using a combination of sensors. 

Attempting to design a system like this can get complicated very quickly - and the sheer scope of possible combinations and adjustments made it difficult to find "the best" configuration. 

### Our Approach: Combining accelerometers and video 
Our two multi-modal approaches data approaches were to use the accelerometers on the mobile device assisted with the camera feed of the individual walking. We also wanted to use the device's gyroscope as well (because other gait recognition systems on mobile devices used it as well), but consistency issues and time constraints prevented us from completing it. 

Furthermore, we use convolutional neural networks (CNN) on the sensor & video feed to perform classification. While it is intuitive to apply CNN's for classifying gaits visually, it might not be intuitive to apply it domains outside of image processing. However, different research groups have tested the effectiveness of CNN's in signal processing and have found moderate success. For example, ["Deep Learning-Based Gait Recognition Using Smartphones in the Wild"](https://arxiv.org/abs/1811.00338) has used CNN's in conjunction with an LSTM to recognize user's through their smartphone. 

### Potential Authentication Schemes
There's really a few ways our classification scheme can approach combining the features. We can just  - we can think of it as "feature concatenation", "ensemble classification", or "pre-trained, fused classification"

For comparison, our base performance consists of only accelerometers and only camera. 

**Potential Authentication Schemes**
Mobile Device + Video through consensus (i.e an ensemble classifier)
Mobile Device + Video through fused classification (feature concatenation)
Mobile Device + Video through combined featurization 

### Security & Confusion Matrix: False Positives, False Negatives, and Other Metrics
We want our authentication system to be as accurate as possible! It should work all the time and be perfectly secure! However, it's important to understand that if perfect accuracy is unachievable, we should decompose how our authentication scheme should behave when it's fails.

This is where I think a "[confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)" provide an easy way of conceptualizing concepts as well as gifting us well defined terminology and evaluation metrics. As a user, the type 1 and type 2 errors ("False Positives" and "False Negatives") greatly affect usability and security. The authentication's usability is affected by the its false negative rate (i.e when it should be authenticating us but isn't) while its vulnerability is proportional to its false positive rate (i.e when it's authenticating things it shouldn't).

##### Silly Examples
Suppose a system has a 0% false positive rate (FPR) and a 25% false negative rate (FNR). Our system would be incredibly secure - there is absolutely no potential for someone to accidentally be identified as you (0% FPR). However, using this would be incredibly annoying. 1 out of every 4 times (25% FNR), the system fails to correctly identify us, and we need to attempt another pass through. 

On the other hand, suppose a system has a 20% FPR and a 0% FNR. This system is very usable, its almost seamless! The system will always be able to identify that it is you (0% FNR). The tradeoff is that a random person could authenticate with your credentials 1 out of 5 times.

##### Does there need to be tradeoffs?
No, there does not have to be - these examples were just to illustrate how you can think about your project and how you can prioritize what you want.

But you can't have have your cake and eat it too! If you try to decrease our type 1 error, you might increase our type 2 error! Vice versa, 



### The Data-Collecting Setup & Our Dataset
Overall, we collected 60 accelerometer traces and video recordings - 20 for each team member. The data was collected using a web-server running on a laptop. Once the subject presses a button on an app, the mobile device and video camera start recording data. After the subject presses the button again, the recording stops and data is sent to the server.  

{% assign caption_arch = "System Architecture" %}
{% include caption_image.html imgpath="/figures/gaitkeep_fig2.png" alt="fig2" caption=caption_arch %}

##### Implementation Logistics 
The server was programmed with Flask using [ngrok tunneling](https://ngrok.com/) to make it accessible over the internet. To start collecting data, we press a button on a custom IOS app that sends a GET request to the web server url url (something like https://{random_string}.ngrok.io/record/{label_name}). 

{% assign caption_coll = "The data collection setup" %}
{% include caption_image.html imgpath="/figures/gaitkeep_fig1.png" alt="fig1" caption=caption_coll %}


A snippet of an accelerometer trace in CSV form. 

| accel_x 	| accel_y 	| accel_z 	|
|:---------:|:---------:|:---------:|
| 149     	| -621    	| -723    	|
| 71      	| -561    	| -796    	|
| 19      	| -390    	| -760    	|
| 79      	| -410    	| -1202   	|
| -56     	| -362    	| -1036   	|
| -106    	| -403    	| -846    	|
| -27     	| -493    	| -786    	|

### Concession: Offloading Computation
Most of the computing, training, and classification was done in a "server" model (i.e run on a random desktop). It's not really *mobile* computing if we offload all our compute in a client-server model. I would consider collecting the information from the phone's sensors as computing, so close enough. 

Offloading compute onto a separate device isn't unheard of - [MAUI](https://dl.acm.org/doi/abs/10.1145/1814433.1814441) researches code offloading for energy preservation, but it's different. In addition there are ways of supporting trained machine learning models on mobile devices. Apple supports machine learning on the IPhone with [CoreML](https://developer.apple.com/documentation/coreml). All of these sophisticated solutions were pretty low priority given the time crunch. 

# Classification
Since our dataset was an equal split between the group, no weighting was associated with the cross-entropy loss; however, this cross entropy loss matters when creating randomized training/validation sets with the training of our GaitNet classifier. 


### Gait Recognition through Accelerometers
The classification model based on accelerometer values was named "GaitNet" - a type of convolutional neural network. The network works by treating data traces with T entries for N features as a T x N matrix and convolving over this matrix with a kernel of different sizes. It then maxpools the output of the convolution layers into two fully-connected layers (like a traditional neural network) - the last layer applies the softmax transfomration our input into a probability distribution of classifications.  

GaitNet used a stochastic gradient descent with a learning rate of 0.001 and a momentum of 0.05. GaitNet was an attempt to be a much shallower and featureless version of [3], but cutting out various features lead to poor results described later. As a result of this poor performance, the reduction of pooling, addition of dropouts, and other tuning tricks rarely led to an increase in performance. 


##### Performance & Reasoning
The mode accuracy of GaitNet's trainings was \~34-35% accuracy - which is no better than randomly guessing. This indicates that GaitNet has no strong predictive power (a classifier that always guessed a single result would maintain that same accuracy rate). 

Interestingly, occasional retraining and testing of GaitNet prediction showed a 70% accuracy rate - indicating that GaitNet could have predicative potential, but these results only occur 10% of the time. These observations could have been just by chance. 

There are several possible factors for the poor performance of GaitNet relating to data-recording configuration, length of traces, and lack of additional features. The explanation behind the performance of GaitNet will be expanded on in the conclusion. 

Something that may improve the performance of GaitNet is to use more features (like gyroscope in each axis) or taking longer traces of the users walking behavior.  


### Gait Recognition Through Video
The vision based classification uses a pretrained model from this [wonderful github repo](https://github.com/marian-margeta/gait-recognition).

The model uses the frams of a video as inputs and outputs an identification vector. 

The identification vectors are well formed and don't seem to work well. 

Two subnetworks in the repository, HumanPoseNN and GaitNN translate the features to human pose descriptors, and uses thse poses in sequence to generate the identifiaction vector. 

Afterward, there is a SVM classifier trained on the idenitifaction vectors in order to clasiffy each other. 

Hyperparameter tuning is done using scikit-learn’s GridSearchCV, which stands for grid search cross validation. The SVC() model from sklearn is used to fit the model to our training data, a sequence of identification vectors and their labels in tuples. We pass in a grid of parameters, and SVC helps us find optimal parameters. Our best regularisation parameter C is 1, the best kernel used to transform data to a higher dimension is gaussian RBF, and the best gamma is 0.001. 


The module which performs visual recognition consists of a pretrained model accessible on GitHub [13], which takes raw RGB video frames as input and outputs an identification vector. The identification vectors for each user’s gait are linearly separable and thus classifiable. Two subnetworks, HumanPoseNN and GaitNN [13], translate spatial features to pose descriptors, then temporal features are pooled into a one-dimensional identification vector. We train a multi-class SVM classifier to distinguish the identification vectors of each registered individual from the others. Hyperparameter tuning is done using scikit-learn’s GridSearchCV, which stands for grid search cross validation. The SVC() model from sklearn is used to fit the model to our training data, a sequence of identification vectors and their labels in tuples. We pass in a grid of parameters, and SVC helps us find optimal parameters. Our best regularisation parameter C is 1, the best kernel used to transform data to a higher dimension is gaussian RBF, and the best gamma is 0.001. 

##### Performance
This model worked very well. 

ID Preciison Recall F1-score

|User ID    | Precision | Recall 	| F1-Score 	|
|:---------:|:---------:|:---------:|:---------:|
| 0     	| 1.00    	| 1.00    	| 1.00 		|
| 1      	| 0.88    	| 1.00    	| 0.93 		|
| 2      	| 1.00    	| 0.83    	| 0.91 		|

|Acc Type   |Results    |
|:---------:|:---------:|
| Accuracy  | 0.94   	| 
| Macro Avg | 0.95   	| 
| Weight Avg| 0.94   	|



# Conclusion & Results

### The Failure of accelerometer identification

When reading about a convolutional approaches to gait recongition through mobile devices, I drastically underestimated the length of data needed, the variety of data (i.e more gyroscopes), and the complexity of the model structure. 

THE MODEL I WROTE IS STRAIGHT TRASH. Whether we needed to collect longer traces or have a more complicated network architecture didn't matter.


##### The Major Problem:
The fact that the accelerometer traces were way too short



##### The Minor Problem
We also didn't include gyroscope information because synchronization broke

# GaitKeep extension and completion?
I think GaitKeep is worth completing, or at least testing the other ideas.

The only dissapointing part I felt from this project is that we never got to fully evaluate the viability of GaitNet given its rushed state. 


I think so - however, the sensor aspect of the problem is very lacking. 

In addition, I think it is worth trying to flaw the initial flaws to further test the performance of various ensemble classification schemes. 


### Footnotes & Additional Papers
<!-- <font size="4em"> -->
[1] OU-ISIR has an amazing biometrics Database - although you need to request for permission http://www.am.sanken.osaka-u.ac.jp/BiometricDB/

[2] https://github.com/marian-margeta/gait-recognition

[3] Q. Zou, Y. Wang, Q. Wang, Y. Zhao and Q. Li, "Deep Learning-Based Gait Recognition Using Smartphones in the Wild," in IEEE Transactions on Information Forensics and Security, vol. 15, pp. 3197-3212, 2020, doi: 10.1109/TIFS.2020.2985628.

[4] M. P. Mufandaidza, T. D. Ramotsoela and G. P. Hancke, "Continuous User Authentication in Smartphones Using Gait Analysis," IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society, Washington, DC, 2018, pp. 4656-4661, doi: 10.1109/IECON.2018.8591193.

[5] Hong Lu, Jonathan Huang, Tanwistha Saha, and Lama Nachman. 2014. Unobtrusive gait verification for mobile phones. In Proceedings of the 2014 ACM International Symposium on Wearable Computers (ISWC ’14). Association for Computing Machinery, New York, NY, USA, 91–98.

[6] Yuanying Chen, Wei Dong, Yi Gao, Xue Liu, and Tao Gu. 2017. Rapid: A Multimodal and Device-free Approach Using Noise Estimation for Robust Person Identification. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 41 (September 2017), 27 pages.

[7] M. P. Mufandaidza, T. D. Ramotsoela and G. P. Hancke, "Continuous User Authentication in Smartphones Using Gait Analysis," IECON 2018 - 44th Annual Conference of the IEEE 
Industrial Electronics Society, Washington, DC, 2018, pp. 4656-4661.

[8] N. Takemura, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi, " Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition", IPSJ Trans. on Computer Vision and Applications, Vol. 10, No. 4, pp. 1-14, Feb. 2018

[9] Hong Lu, Jonathan Huang, Tanwistha Saha, and Lama Nachman. 2014. Unobtrusive gait verification for mobile phones. In Proceedings of the 2014 ACM International Symposium on Wearable Computers (ISWC ’14). Association for Computing Machinery, New York, NY, USA, 91–98. DOI:https://doi.org/10.1145/2634317.2642868

[10]  A. Jain, A. Ross, "An introduction to biometric recognition", IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 1, pp. 4-20, 2004.

[11] Gafurov, D.: A survey of biometric gait recognition: Approaches, security and challenges. In: NIK conference (2007)

[12] P. J. Phillips, A. Martin, C. L. Wilson and M. Przybocki, "An introduction evaluating biometric systems," in Computer, vol. 33, no. 2, pp. 56-63, Feb. 2000.

[13] Emiliano Miluzzo, Nicholas D. Lane, Kristóf Fodor, Ronald Peterson, Hong Lu, Mirco Musolesi, Shane B. Eisenman, Xiao Zheng, and Andrew T. Campbell. 2008. Sensing meets mobile social networks: the design, implementation and evaluation of the CenceMe application. In Proceedings of the 6th ACM conference on Embedded network sensor systems (SenSys ’08). Association for Computing Machinery, New York, NY, USA, 337–350. 

[14] S. Eberz, G. Lovisotto, A. Patanè, M. Kwiatkowska, V. Lenders and I. Martinovic, "When Your Fitness Tracker Betrays You: Quantifying the Predictability of Biometric Features Across Contexts," 2018 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, 2018, pp. 889-905.

[15] Huan Feng, Kassem Fawaz, and Kang G. Shin. 2017. Continuous Authentication for Voice Assistants. In Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking (MobiCom ’17). Association for Computing Machinery, New York, NY, USA, 343–355.

[16] Eduardo Cuervo, Aruna Balasubramanian, Dae-ki Cho, Alec Wolman, Stefan Saroiu, Ranveer Chandra, and Paramvir Bahl. 2010. MAUI: making smartphones last longer with code offload. In Proceedings of the 8th international conference on Mobile systems, applications, and services (MobiSys ’10). Association for Computing Machinery, New York, NY, USA, 49–62.

[17] Tiffany Yu-Han Chen, Hari Balakrishnan, Lenin Ravindranath, and Paramvir Bahl. 2016. GLIMPSE: Continuous, Real-Time Object Recognition on Mobile Devices. GetMobile: Mobile Comp. and Comm. 20, 1 (July 2016), 26–29.

[18] S. R. Das, R. C. Wilson, M. T. Lazarewicz and L. H. Finkel, "Gait recognition by two-stage principal component analysis," 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Southampton, 2006, pp. 579-584, doi: 10.1109/FGR.2006.56.

[19] Valentin Radu, Catherine Tong, Sourav Bhattacharya, Nicholas D. Lane, Cecilia Mascolo, Mahesh K. Marina, and Fahim Kawsar. 2018. Multimodal Deep Learning for Activity and Context Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4, Article 157 (December 2017), 27 pages. DOI:https://doi.org/10.1145/3161174
<!-- </font> -->